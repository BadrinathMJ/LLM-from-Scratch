{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pdfminer\n",
      "  Downloading pdfminer-20191125.tar.gz (4.2 MB)\n",
      "     ---------------------------------------- 0.0/4.2 MB ? eta -:--:--\n",
      "     ---------------------------------------- 0.0/4.2 MB ? eta -:--:--\n",
      "     ---------------------------------------- 0.0/4.2 MB ? eta -:--:--\n",
      "     ---------------------------------------- 0.0/4.2 MB ? eta -:--:--\n",
      "     ---------------------------------------- 0.0/4.2 MB 145.2 kB/s eta 0:00:29\n",
      "     ---------------------------------------- 0.0/4.2 MB 145.2 kB/s eta 0:00:29\n",
      "      --------------------------------------- 0.1/4.2 MB 192.5 kB/s eta 0:00:22\n",
      "      --------------------------------------- 0.1/4.2 MB 206.9 kB/s eta 0:00:20\n",
      "     - -------------------------------------- 0.1/4.2 MB 284.4 kB/s eta 0:00:15\n",
      "     - -------------------------------------- 0.2/4.2 MB 388.2 kB/s eta 0:00:11\n",
      "     - -------------------------------------- 0.2/4.2 MB 428.5 kB/s eta 0:00:10\n",
      "     -- ------------------------------------- 0.2/4.2 MB 450.6 kB/s eta 0:00:09\n",
      "     -- ------------------------------------- 0.3/4.2 MB 505.4 kB/s eta 0:00:08\n",
      "     --- ------------------------------------ 0.3/4.2 MB 551.6 kB/s eta 0:00:07\n",
      "     ---- ----------------------------------- 0.5/4.2 MB 685.8 kB/s eta 0:00:06\n",
      "     ----- ---------------------------------- 0.6/4.2 MB 829.1 kB/s eta 0:00:05\n",
      "     ------ --------------------------------- 0.7/4.2 MB 942.1 kB/s eta 0:00:04\n",
      "     --------- ------------------------------ 1.0/4.2 MB 1.2 MB/s eta 0:00:03\n",
      "     ------------ --------------------------- 1.3/4.2 MB 1.5 MB/s eta 0:00:02\n",
      "     -------------- ------------------------- 1.5/4.2 MB 1.7 MB/s eta 0:00:02\n",
      "     -------------------- ------------------- 2.1/4.2 MB 2.2 MB/s eta 0:00:01\n",
      "     -------------------------- ------------- 2.7/4.2 MB 2.7 MB/s eta 0:00:01\n",
      "     ----------------------------- ---------- 3.1/4.2 MB 2.9 MB/s eta 0:00:01\n",
      "     ---------------------------------------  4.2/4.2 MB 3.8 MB/s eta 0:00:01\n",
      "     ---------------------------------------- 4.2/4.2 MB 3.6 MB/s eta 0:00:00\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Collecting pycryptodome (from pdfminer)\n",
      "  Downloading pycryptodome-3.19.0-cp35-abi3-win_amd64.whl.metadata (3.4 kB)\n",
      "Downloading pycryptodome-3.19.0-cp35-abi3-win_amd64.whl (1.7 MB)\n",
      "   ---------------------------------------- 0.0/1.7 MB ? eta -:--:--\n",
      "   ---------------------------------------  1.7/1.7 MB 55.5 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 1.7/1.7 MB 22.2 MB/s eta 0:00:00\n",
      "Building wheels for collected packages: pdfminer\n",
      "  Building wheel for pdfminer (setup.py): started\n",
      "  Building wheel for pdfminer (setup.py): finished with status 'done'\n",
      "  Created wheel for pdfminer: filename=pdfminer-20191125-py3-none-any.whl size=6140142 sha256=cf38360c7b77b4c7e08564f167afaa0e23c3082b117251cf26639c527340fdaa\n",
      "  Stored in directory: c:\\users\\mayur\\appdata\\local\\pip\\cache\\wheels\\4e\\c1\\68\\f7bd0a8f514661f76b5cbe3b5f76e0033d79f1296012cbbf72\n",
      "Successfully built pdfminer\n",
      "Installing collected packages: pycryptodome, pdfminer\n",
      "Successfully installed pdfminer-20191125 pycryptodome-3.19.0\n"
     ]
    }
   ],
   "source": [
    "!pip install pdfminer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pdfminer.six\n",
      "  Downloading pdfminer.six-20221105-py3-none-any.whl (5.6 MB)\n",
      "     ---------------------------------------- 0.0/5.6 MB ? eta -:--:--\n",
      "     ---------------------------------------- 0.0/5.6 MB ? eta -:--:--\n",
      "     ---------------------------------------- 0.0/5.6 MB ? eta -:--:--\n",
      "     ---------------------------------------- 0.0/5.6 MB 220.2 kB/s eta 0:00:26\n",
      "     ---------------------------------------- 0.0/5.6 MB 220.2 kB/s eta 0:00:26\n",
      "     ---------------------------------------- 0.1/5.6 MB 297.7 kB/s eta 0:00:19\n",
      "      --------------------------------------- 0.1/5.6 MB 328.8 kB/s eta 0:00:17\n",
      "      --------------------------------------- 0.1/5.6 MB 425.1 kB/s eta 0:00:13\n",
      "     - -------------------------------------- 0.2/5.6 MB 492.3 kB/s eta 0:00:12\n",
      "     - -------------------------------------- 0.2/5.6 MB 567.2 kB/s eta 0:00:10\n",
      "     - -------------------------------------- 0.2/5.6 MB 603.9 kB/s eta 0:00:09\n",
      "     -- ------------------------------------- 0.3/5.6 MB 632.7 kB/s eta 0:00:09\n",
      "     -- ------------------------------------- 0.3/5.6 MB 677.0 kB/s eta 0:00:08\n",
      "     -- ------------------------------------- 0.4/5.6 MB 675.0 kB/s eta 0:00:08\n",
      "     --- ------------------------------------ 0.5/5.6 MB 863.4 kB/s eta 0:00:06\n",
      "     ----- ---------------------------------- 0.7/5.6 MB 1.1 MB/s eta 0:00:05\n",
      "     ------ --------------------------------- 0.9/5.6 MB 1.2 MB/s eta 0:00:04\n",
      "     --------- ------------------------------ 1.4/5.6 MB 1.7 MB/s eta 0:00:03\n",
      "     ----------- ---------------------------- 1.6/5.6 MB 2.0 MB/s eta 0:00:03\n",
      "     ---------------- ----------------------- 2.3/5.6 MB 2.6 MB/s eta 0:00:02\n",
      "     -------------------- ------------------- 2.9/5.6 MB 3.1 MB/s eta 0:00:01\n",
      "     ------------------------- -------------- 3.6/5.6 MB 3.6 MB/s eta 0:00:01\n",
      "     ---------------------------------- ----- 4.9/5.6 MB 4.6 MB/s eta 0:00:01\n",
      "     ---------------------------------------  5.5/5.6 MB 5.1 MB/s eta 0:00:01\n",
      "     ---------------------------------------- 5.6/5.6 MB 4.9 MB/s eta 0:00:00\n",
      "Collecting charset-normalizer>=2.0.0 (from pdfminer.six)\n",
      "  Downloading charset_normalizer-3.3.2-cp310-cp310-win_amd64.whl.metadata (34 kB)\n",
      "Collecting cryptography>=36.0.0 (from pdfminer.six)\n",
      "  Downloading cryptography-41.0.7-cp37-abi3-win_amd64.whl.metadata (5.3 kB)\n",
      "Collecting cffi>=1.12 (from cryptography>=36.0.0->pdfminer.six)\n",
      "  Downloading cffi-1.16.0-cp310-cp310-win_amd64.whl.metadata (1.5 kB)\n",
      "Collecting pycparser (from cffi>=1.12->cryptography>=36.0.0->pdfminer.six)\n",
      "  Using cached pycparser-2.21-py2.py3-none-any.whl (118 kB)\n",
      "Downloading charset_normalizer-3.3.2-cp310-cp310-win_amd64.whl (100 kB)\n",
      "   ---------------------------------------- 0.0/100.3 kB ? eta -:--:--\n",
      "   ---------------------------------------- 100.3/100.3 kB 5.6 MB/s eta 0:00:00\n",
      "Downloading cryptography-41.0.7-cp37-abi3-win_amd64.whl (2.7 MB)\n",
      "   ---------------------------------------- 0.0/2.7 MB ? eta -:--:--\n",
      "   ----------------------------- ---------- 1.9/2.7 MB 62.4 MB/s eta 0:00:01\n",
      "   ------------------------------ --------- 2.0/2.7 MB 42.9 MB/s eta 0:00:01\n",
      "   ------------------------------ --------- 2.0/2.7 MB 42.9 MB/s eta 0:00:01\n",
      "   ------------------------------ --------- 2.0/2.7 MB 42.9 MB/s eta 0:00:01\n",
      "   ---------------------------------------  2.7/2.7 MB 13.1 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 2.7/2.7 MB 10.6 MB/s eta 0:00:00\n",
      "Downloading cffi-1.16.0-cp310-cp310-win_amd64.whl (181 kB)\n",
      "   ---------------------------------------- 0.0/181.6 kB ? eta -:--:--\n",
      "   -------------------------------------- - 174.1/181.6 kB 5.3 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 181.6/181.6 kB 2.8 MB/s eta 0:00:00\n",
      "Installing collected packages: pycparser, charset-normalizer, cffi, cryptography, pdfminer.six\n",
      "Successfully installed cffi-1.16.0 charset-normalizer-3.3.2 cryptography-41.0.7 pdfminer.six-20221105 pycparser-2.21\n",
      "Collecting PyPDF2\n",
      "  Downloading pypdf2-3.0.1-py3-none-any.whl (232 kB)\n",
      "     ---------------------------------------- 0.0/232.6 kB ? eta -:--:--\n",
      "     - -------------------------------------- 10.2/232.6 kB ? eta -:--:--\n",
      "     - -------------------------------------- 10.2/232.6 kB ? eta -:--:--\n",
      "     ---- -------------------------------- 30.7/232.6 kB 217.9 kB/s eta 0:00:01\n",
      "     ---- -------------------------------- 30.7/232.6 kB 217.9 kB/s eta 0:00:01\n",
      "     ------ ------------------------------ 41.0/232.6 kB 178.6 kB/s eta 0:00:02\n",
      "     ------------- ----------------------- 81.9/232.6 kB 306.3 kB/s eta 0:00:01\n",
      "     ----------------- ------------------ 112.6/232.6 kB 364.4 kB/s eta 0:00:01\n",
      "     ------------------------- ---------- 163.8/232.6 kB 446.5 kB/s eta 0:00:01\n",
      "     ------------------------------- ---- 204.8/232.6 kB 497.6 kB/s eta 0:00:01\n",
      "     ------------------------------------ 232.6/232.6 kB 526.9 kB/s eta 0:00:00\n",
      "Installing collected packages: PyPDF2\n",
      "Successfully installed PyPDF2-3.0.1\n"
     ]
    }
   ],
   "source": [
    "!pip install pdfminer.six\n",
    "!pip install PyPDF2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Convert PDF to TEXT**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pdfminer.high_level import extract_text\n",
    "from pdfminer.pdfparser import PDFParser\n",
    "from pdfminer.pdfdocument import PDFDocument\n",
    "from pdfminer.pdfpage import PDFPage\n",
    "import re\n",
    "import os\n",
    "import shutil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File'Maheshwar_Mandal_Anr_vs_The_State_Of_Bihar_Ors_on_24_June_2014 (1).PDF' successfully copied to 'Maheshwar_Mandal_Anr_vs_The_State_Of_Bihar_Ors_on_24_June_2014 (1)'\n"
     ]
    }
   ],
   "source": [
    "doc_name = \"Maheshwar_Mandal_Anr_vs_The_State_Of_Bihar_Ors_on_24_June_2014 (1)\"\n",
    "\n",
    "process_folder = doc_name\n",
    "\n",
    "if not os.path.exists(process_folder):\n",
    "    os.makedirs(process_folder)\n",
    "\n",
    "path = process_folder + \"\\\\\" + doc_name + \".PDF\"\n",
    "textfile_name = process_folder + \"\\\\\" + doc_name + \".txt\"\n",
    "\n",
    "#Attempt to copy the PDF\n",
    "try:\n",
    "    shutil.copy(f\"{doc_name}.PDF\", process_folder)\n",
    "    print(f\"File'{doc_name}.PDF' successfully copied to '{process_folder}'\")\n",
    "except FileNotFoundError:\n",
    "    print(f\"File '{doc_name}.PDF not found. Please download it from resource area\")\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred{e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Save entire text to text file\n",
    "import os   \n",
    "from PyPDF2 import PdfReader\n",
    "\n",
    "document = PdfReader(open(path, \"rb\"))   #replace with you pdf file\n",
    "\n",
    "all_pages_text = []\n",
    "page_num = 0\n",
    "\n",
    "for i in range(len(document.pages)):\n",
    "    #Convert page to pdf file writer object\n",
    "    page = document.pages[i]\n",
    "\n",
    "    #Extract text from page\n",
    "    page_text = page.extract_text()\n",
    "\n",
    "    start_page = i - 1\n",
    "    all_pages_text.append(page_text)\n",
    "\n",
    "with open(textfile_name, 'w',encoding=\"utf-8\") as file:\n",
    "    for item in all_pages_text:\n",
    "        file.write(str(item)+'\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Split text by sentence**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pandas\n",
      "  Using cached pandas-2.1.3-cp310-cp310-win_amd64.whl.metadata (18 kB)\n",
      "Requirement already satisfied: numpy<2,>=1.22.4 in d:\\users\\mayur1\\envs\\llm_from_scratch\\lib\\site-packages (from pandas) (1.26.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in d:\\users\\mayur1\\envs\\llm_from_scratch\\lib\\site-packages (from pandas) (2.8.2)\n",
      "Collecting pytz>=2020.1 (from pandas)\n",
      "  Using cached pytz-2023.3.post1-py2.py3-none-any.whl.metadata (22 kB)\n",
      "Collecting tzdata>=2022.1 (from pandas)\n",
      "  Using cached tzdata-2023.3-py2.py3-none-any.whl (341 kB)\n",
      "Requirement already satisfied: six>=1.5 in d:\\users\\mayur1\\envs\\llm_from_scratch\\lib\\site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
      "Using cached pandas-2.1.3-cp310-cp310-win_amd64.whl (10.7 MB)\n",
      "Using cached pytz-2023.3.post1-py2.py3-none-any.whl (502 kB)\n",
      "Installing collected packages: pytz, tzdata, pandas\n",
      "Successfully installed pandas-2.1.3 pytz-2023.3.post1 tzdata-2023.3\n"
     ]
    }
   ],
   "source": [
    "!pip install pandas\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting nltk\n",
      "  Downloading nltk-3.8.1-py3-none-any.whl (1.5 MB)\n",
      "     ---------------------------------------- 0.0/1.5 MB ? eta -:--:--\n",
      "      --------------------------------------- 0.0/1.5 MB 1.4 MB/s eta 0:00:02\n",
      "      --------------------------------------- 0.0/1.5 MB 1.4 MB/s eta 0:00:02\n",
      "      --------------------------------------- 0.0/1.5 MB 1.4 MB/s eta 0:00:02\n",
      "     -- ------------------------------------- 0.1/1.5 MB 459.5 kB/s eta 0:00:04\n",
      "     -- ------------------------------------- 0.1/1.5 MB 656.4 kB/s eta 0:00:03\n",
      "     ---- ----------------------------------- 0.2/1.5 MB 612.6 kB/s eta 0:00:03\n",
      "     ----- ---------------------------------- 0.2/1.5 MB 765.3 kB/s eta 0:00:02\n",
      "     ------ --------------------------------- 0.2/1.5 MB 761.1 kB/s eta 0:00:02\n",
      "     ------- -------------------------------- 0.3/1.5 MB 770.1 kB/s eta 0:00:02\n",
      "     --------- ------------------------------ 0.4/1.5 MB 851.3 kB/s eta 0:00:02\n",
      "     ---------- ----------------------------- 0.4/1.5 MB 809.6 kB/s eta 0:00:02\n",
      "     -------------- ------------------------- 0.5/1.5 MB 1.0 MB/s eta 0:00:01\n",
      "     ---------------- ----------------------- 0.6/1.5 MB 1.1 MB/s eta 0:00:01\n",
      "     ---------------------- ----------------- 0.9/1.5 MB 1.4 MB/s eta 0:00:01\n",
      "     ------------------------------- -------- 1.2/1.5 MB 1.8 MB/s eta 0:00:01\n",
      "     --------------------------------- ------ 1.3/1.5 MB 1.7 MB/s eta 0:00:01\n",
      "     ---------------------------------------  1.5/1.5 MB 2.0 MB/s eta 0:00:01\n",
      "     ---------------------------------------- 1.5/1.5 MB 1.9 MB/s eta 0:00:00\n",
      "Collecting click (from nltk)\n",
      "  Downloading click-8.1.7-py3-none-any.whl.metadata (3.0 kB)\n",
      "Collecting joblib (from nltk)\n",
      "  Using cached joblib-1.3.2-py3-none-any.whl.metadata (5.4 kB)\n",
      "Requirement already satisfied: regex>=2021.8.3 in d:\\users\\mayur1\\envs\\llm_from_scratch\\lib\\site-packages (from nltk) (2023.10.3)\n",
      "Requirement already satisfied: tqdm in d:\\users\\mayur1\\envs\\llm_from_scratch\\lib\\site-packages (from nltk) (4.66.1)\n",
      "Requirement already satisfied: colorama in d:\\users\\mayur1\\envs\\llm_from_scratch\\lib\\site-packages (from click->nltk) (0.4.6)\n",
      "Downloading click-8.1.7-py3-none-any.whl (97 kB)\n",
      "   ---------------------------------------- 0.0/97.9 kB ? eta -:--:--\n",
      "   ---------------------------------------- 97.9/97.9 kB 2.8 MB/s eta 0:00:00\n",
      "Using cached joblib-1.3.2-py3-none-any.whl (302 kB)\n",
      "Installing collected packages: joblib, click, nltk\n",
      "Successfully installed click-8.1.7 joblib-1.3.2 nltk-3.8.1\n"
     ]
    }
   ],
   "source": [
    "!pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Mayur\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "\n",
    "# Download the punkt tokenizer models\n",
    "nltk.download('punkt')\n",
    "\n",
    "def file_to_sentences(filepath):\n",
    "    \"\"\"\n",
    "    Given filepath, read text file and split it into sentences.\n",
    "    Returns the list of sentences\n",
    "    \"\"\"\n",
    "    with open(filepath, 'r', encoding='utf-8') as file:\n",
    "        content = file.read()\n",
    "        sentences = nltk.tokenize.sent_tokenize(content)\n",
    "    return sentences\n",
    "\n",
    "def add_sentences_to_dataframe(sentences, dataframe):\n",
    "    \"\"\"\n",
    "    Add sentences to given dataframe\n",
    "    \"\"\"\n",
    "    for sentence in sentences:\n",
    "        dataframe = dataframe.append({'Sentence': sentence}, ignore_index=True)\n",
    "\n",
    "#Initialize the empty dataframe with one column \"sentence\"\n",
    "df = pd.DataFrame(columns=['text'])\n",
    "\n",
    "#Initializes a list holds all sentencs\n",
    "all_sentences = []\n",
    "\n",
    "#Set the dictionary where the text files are located\n",
    "filepath = f\"{doc_name}/{doc_name}.txt\"\n",
    "all_sentences.extend(file_to_sentences(filepath))\n",
    "\n",
    "#Convert the list of sentences to a Dataframe\n",
    "df = pd.DataFrame(all_sentences, columns=['Sentence'])\n",
    "\n",
    "#Save to csv\n",
    "df.to_csv(f\"{doc_name}_sentences.csv\",index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **PRE-PROCESS THE TEXT**\n",
    "\n",
    "Load as unicode (which is usually the default of Python anyways)\n",
    "\n",
    "Lowercase the entire text\n",
    "\n",
    "Exclude all symbols except these: . , ; ? ! ' \""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'Maheshwar_Mandal_Anr_vs_The_State_Of_Bihar_Ors_on_24_June_2014 (1) \\\\ Maheshwar_Mandal_Anr_vs_The_State_Of_Bihar_Ors_on_24_June_2014 (1).txt'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32md:\\My_Practice_LLM\\app_preprocessing.ipynb Cell 12\u001b[0m line \u001b[0;36m2\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/My_Practice_LLM/app_preprocessing.ipynb#X13sZmlsZQ%3D%3D?line=18'>19</a>\u001b[0m \u001b[39m#Read the content of the file \u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/My_Practice_LLM/app_preprocessing.ipynb#X13sZmlsZQ%3D%3D?line=19'>20</a>\u001b[0m filepath \u001b[39m=\u001b[39m \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00mdoc_name\u001b[39m}\u001b[39;00m\u001b[39m \u001b[39m\u001b[39m\\\u001b[39m\u001b[39m \u001b[39m\u001b[39m{\u001b[39;00mdoc_name\u001b[39m}\u001b[39;00m\u001b[39m.txt\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m---> <a href='vscode-notebook-cell:/d%3A/My_Practice_LLM/app_preprocessing.ipynb#X13sZmlsZQ%3D%3D?line=21'>22</a>\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mopen\u001b[39;49m(filepath, \u001b[39m'\u001b[39;49m\u001b[39mr\u001b[39;49m\u001b[39m'\u001b[39;49m, encoding\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39mutf-8\u001b[39;49m\u001b[39m'\u001b[39;49m) \u001b[39mas\u001b[39;00m file:\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/My_Practice_LLM/app_preprocessing.ipynb#X13sZmlsZQ%3D%3D?line=22'>23</a>\u001b[0m     content \u001b[39m=\u001b[39m file\u001b[39m.\u001b[39mread()\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/My_Practice_LLM/app_preprocessing.ipynb#X13sZmlsZQ%3D%3D?line=24'>25</a>\u001b[0m \u001b[39m#Clean the content\u001b[39;00m\n",
      "File \u001b[1;32md:\\Users\\Mayur1\\envs\\LLM_from_scratch\\lib\\site-packages\\IPython\\core\\interactiveshell.py:310\u001b[0m, in \u001b[0;36m_modified_open\u001b[1;34m(file, *args, **kwargs)\u001b[0m\n\u001b[0;32m    303\u001b[0m \u001b[39mif\u001b[39;00m file \u001b[39min\u001b[39;00m {\u001b[39m0\u001b[39m, \u001b[39m1\u001b[39m, \u001b[39m2\u001b[39m}:\n\u001b[0;32m    304\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[0;32m    305\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mIPython won\u001b[39m\u001b[39m'\u001b[39m\u001b[39mt let you open fd=\u001b[39m\u001b[39m{\u001b[39;00mfile\u001b[39m}\u001b[39;00m\u001b[39m by default \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    306\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mas it is likely to crash IPython. If you know what you are doing, \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    307\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39myou can use builtins\u001b[39m\u001b[39m'\u001b[39m\u001b[39m open.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    308\u001b[0m     )\n\u001b[1;32m--> 310\u001b[0m \u001b[39mreturn\u001b[39;00m io_open(file, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'Maheshwar_Mandal_Anr_vs_The_State_Of_Bihar_Ors_on_24_June_2014 (1) \\\\ Maheshwar_Mandal_Anr_vs_The_State_Of_Bihar_Ors_on_24_June_2014 (1).txt'"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import unicodedata\n",
    "\n",
    "#Function to convert ASCII\n",
    "def to_ascii(text):\n",
    "    normalized = unicodedata.normalize('NFKD', text)\n",
    "    return normalized.encode('ascii','ignore').decode('ascii')\n",
    "\n",
    "#Function to clean the text\n",
    "def clean_text(text):\n",
    "    #Convert to lowercase and to ASCII\n",
    "    text = to_ascii(text.lower())\n",
    "    #Keep only alphabetic characters and spaces\n",
    "    text = re.sub(r'[^a-z\\s]+',' ', text)\n",
    "    #Normalize spaces to single space\n",
    "    text = re.sub(r'\\s+',' ', text).strip()\n",
    "    return text\n",
    "\n",
    "#Read the content of the file \n",
    "filepath = f\"{doc_name}\\{doc_name}.txt\"\n",
    "\n",
    "with open(filepath, 'r', encoding='utf-8') as file:\n",
    "    content = file.read()\n",
    "\n",
    "#Clean the content\n",
    "cleaned_content = clean_text(content)\n",
    "\n",
    "#Save the cleaned content back to the file \n",
    "filepath_cleaned = f\"{doc_name}\\{doc_name}_clean.txt\"\n",
    "with open(filepath_cleaned, 'w', encoding='ascii') as file:\n",
    "    file.write(cleaned_content)\n",
    "\n",
    "print(f\"The text has been cleaned and saved to {filepath_cleaned}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "LLM_from_scratch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
